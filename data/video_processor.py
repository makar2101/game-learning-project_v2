"""
Video Processor - –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª—ñ–≤
–°–∫–∞–Ω—É—î –ø–∞–ø–∫—É, –ø–µ—Ä–µ–≤—ñ—Ä—è—î –∑–º—ñ–Ω–∏, –≤–∏—Ç—è–≥—É—î –∞—É–¥—ñ–æ —Ç–∞ —Å—Ç–≤–æ—Ä—é—î —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import time
from datetime import datetime

# –Ü–º–ø–æ—Ä—Ç–∏ –∑ —ñ—Å–Ω—É—é—á–∏—Ö –º–æ–¥—É–ª—ñ–≤
from processing.audio_extractor import AudioExtractor
from processing.transcriber import Transcriber
from processing.database_manager import DatabaseManager
from data.data_manager import DataManager

class VideoProcessor:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –æ–±—Ä–æ–±–∫–∞ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª—ñ–≤"""
    
    def __init__(self, 
                 videos_dir: str = "videos",
                 supported_formats: List[str] = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è Video Processor
        
        Args:
            videos_dir: –ü–∞–ø–∫–∞ –∑ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª–∞–º–∏
            supported_formats: –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏ –≤—ñ–¥–µ–æ
        """
        self.videos_dir = Path(videos_dir)
        self.supported_formats = supported_formats or ['.mkv', '.mp4', '.avi', '.mov', '.wmv']
        
        self.logger = logging.getLogger(__name__)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤
        self.audio_extractor = AudioExtractor()
        self.transcriber = Transcriber(model_size='tiny', device='cpu')
        self.db_manager = DatabaseManager()
        self.data_manager = DataManager()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–æ–±–∫–∏
        self.processing_stats = {
            "videos_found": 0,
            "videos_new": 0,
            "videos_changed": 0,
            "videos_processed": 0,
            "videos_failed": 0,
            "sentences_extracted": 0,
            "processing_time": 0.0
        }
    
    def scan_videos_directory(self) -> List[Dict]:
        """
        –°–∫–∞–Ω—É—î –ø–∞–ø–∫—É –∑ –≤—ñ–¥–µ–æ —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª—ñ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –≤—ñ–¥–µ–æ
        """
        video_files = []
        
        if not self.videos_dir.exists():
            self.logger.warning(f"–ü–∞–ø–∫–∞ –∑ –≤—ñ–¥–µ–æ –Ω–µ —ñ—Å–Ω—É—î: {self.videos_dir}")
            return video_files
        
        # –®—É–∫–∞—î–º–æ –≤—Å—ñ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª–∏
        for file_path in self.videos_dir.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in self.supported_formats:
                try:
                    stat = file_path.stat()
                    
                    video_info = {
                        "filename": file_path.name,
                        "filepath": str(file_path),
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(stat.st_mtime),
                        "extension": file_path.suffix.lower()
                    }
                    
                    video_files.append(video_info)
                    
                except Exception as e:
                    self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è —Ñ–∞–π–ª—É {file_path}: {e}")
        
        self.processing_stats["videos_found"] = len(video_files)
        self.logger.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(video_files)} –≤—ñ–¥–µ–æ —Ñ–∞–π–ª—ñ–≤")
        
        return sorted(video_files, key=lambda x: x["modified"], reverse=True)
    
    def check_video_changes(self, video_info: Dict) -> str:
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î —Å—Ç–∞–Ω –≤—ñ–¥–µ–æ —Ñ–∞–π–ª—É
        
        Args:
            video_info: –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≤—ñ–¥–µ–æ
            
        Returns:
            'new', 'changed', 'processed', 'failed'
        """
        filename = video_info["filename"]
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–∞–Ω –≤ data_manager
        video_state = self.data_manager.get_video_state(filename)
        
        if not video_state:
            return 'new'
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∑–º—ñ–Ω–∏–≤—Å—è —Ñ–∞–π–ª
        current_hash = self.data_manager._get_file_hash(video_info["filepath"])
        
        if current_hash != video_state["file_hash"]:
            return 'changed'
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–æ–±–∫–∞
        if video_state["processing_completed"]:
            return 'processed'
        
        return 'failed'

    def split_text_into_sentences(self, segments: List[Dict]) -> List[Dict]:
        """
        –†–æ–∑–±–∏–≤–∞—î —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó –Ω–∞ –æ–∫—Ä–µ–º—ñ —Ä–µ—á–µ–Ω–Ω—è –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é
        
        Args:
            segments: –°–µ–≥–º–µ–Ω—Ç–∏ –∑ Whisper –∞–±–æ –ë–î
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ—á–µ–Ω—å –∑ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏ –±–µ–∑ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        """
        sentences = []
        seen_sentences = set()  # –î–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å
        
        for segment in segments:
            text = segment['text'].strip()
            
            # –£–ù–Ü–í–ï–†–°–ê–õ–¨–ù–ï –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ø—ñ–¥—Ç—Ä–∏–º—É—î–º–æ –æ–±–∏–¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∏
            if 'start_time' in segment:
                # –§–æ—Ä–º–∞—Ç –∑ –ë–î
                start_time = segment['start_time']
                end_time = segment['end_time']
            elif 'start' in segment:
                # –§–æ—Ä–º–∞—Ç –∑ Whisper
                start_time = segment['start']
                end_time = segment['end']
            else:
                self.logger.error(f"–°–µ–≥–º–µ–Ω—Ç –Ω–µ –º–∞—î –ø–æ–ª—ñ–≤ —á–∞—Å—É: {list(segment.keys())}")
                continue
            
            # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö
            # –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ '.', '!', '?' –∞–ª–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —Ü—ñ —Å–∏–º–≤–æ–ª–∏
            sentence_pattern = r'([.!?]+)'
            parts = re.split(sentence_pattern, text)
            
            current_sentence = ""
            sentence_start = start_time
            
            for i, part in enumerate(parts):
                if part.strip():
                    current_sentence += part
                    
                    # –Ø–∫—â–æ —Ü–µ –∫—ñ–Ω–µ—Ü—å —Ä–µ—á–µ–Ω–Ω—è (–ø—É–Ω–∫—Ç—É–∞—Ü—ñ—è)
                    if re.match(sentence_pattern, part):
                        if current_sentence.strip():
                            # –û–±—á–∏—Å–ª—é—î–º–æ –ø—Ä–∏–±–ª–∏–∑–Ω–∏–π —á–∞—Å –∫—ñ–Ω—Ü—è —Ä–µ—á–µ–Ω–Ω—è
                            progress = (i + 1) / len(parts)
                            sentence_end = sentence_start + (end_time - sentence_start) * progress
                            
                            # –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏
                            normalized_text = self._normalize_sentence_for_comparison(current_sentence.strip())
                            
                            if normalized_text not in seen_sentences:
                                seen_sentences.add(normalized_text)
                                
                                sentences.append({
                                    'text': current_sentence.strip(),
                                    'start_time': float(sentence_start),
                                    'end_time': float(min(sentence_end, end_time)),
                                    'confidence': segment.get('confidence', 0.0),
                                    'normalized_text': normalized_text  # –î–ª—è –ø–æ–¥–∞–ª—å—à–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
                                })
                            else:
                                self.logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç: {current_sentence.strip()[:30]}...")
                            
                            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ –Ω–∞—Å—Ç—É–ø–Ω–æ–≥–æ —Ä–µ—á–µ–Ω–Ω—è
                            current_sentence = ""
                            sentence_start = sentence_end
            
            # –Ø–∫—â–æ –∑–∞–ª–∏—à–∏–≤—Å—è —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—ñ–Ω—Ü–µ–≤–æ—ó –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó
            if current_sentence.strip():
                normalized_text = self._normalize_sentence_for_comparison(current_sentence.strip())
                
                if normalized_text not in seen_sentences:
                    seen_sentences.add(normalized_text)
                    
                    sentences.append({
                        'text': current_sentence.strip(),
                        'start_time': float(sentence_start),
                        'end_time': float(end_time),
                        'confidence': segment.get('confidence', 0.0),
                        'normalized_text': normalized_text
                    })
                else:
                    self.logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç (–∫—ñ–Ω—Ü–µ–≤–∏–π): {current_sentence.strip()[:30]}...")
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥—É–∂–µ –∫–æ—Ä–æ—Ç–∫—ñ —Ä–µ—á–µ–Ω–Ω—è
        filtered_sentences = []
        for sentence in sentences:
            text = sentence['text'].strip()
            # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ä–µ—á–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç—à—ñ –∑–∞ 10 —Å–∏–º–≤–æ–ª—ñ–≤ –∞–±–æ —â–æ –º—ñ—Å—Ç—è—Ç—å —Ç—ñ–ª—å–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é
            if len(text) >= 10 and not re.match(r'^[.!?\s]+$', text):
                # –í–∏–¥–∞–ª—è—î–º–æ normalized_text –ø–µ—Ä–µ–¥ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è–º (–Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω —É UI)
                del sentence['normalized_text']
                filtered_sentences.append(sentence)
        
        self.logger.info(f"–î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è: {len(sentences)} ‚Üí {len(filtered_sentences)} —Ä–µ—á–µ–Ω—å")
        
        return filtered_sentences

    def _normalize_sentence_for_comparison(self, text: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª—ñ–∑—É—î —Ä–µ—á–µ–Ω–Ω—è –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –Ω–∞ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        
        Args:
            text: –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç —Ä–µ—á–µ–Ω–Ω—è
            
        Returns:
            –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        """
        import re
        
        # –ü—Ä–∏–≤–æ–¥–∏–º–æ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É
        normalized = text.lower()
        
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
        normalized = ' '.join(normalized.split())
        
        # –í–∏–¥–∞–ª—è—î–º–æ –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—é –∑ –∫—ñ–Ω—Ü—ñ–≤
        normalized = re.sub(r'^[^\w]+|[^\w]+$', '', normalized)
        
        # –ó–∞–º—ñ–Ω—é—î–º–æ –º–Ω–æ–∂–∏–Ω–Ω—ñ –ø—Ä–æ–±—ñ–ª–∏ –æ–¥–Ω–∏–º
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # –í–∏–¥–∞–ª—è—î–º–æ –¥–µ—è–∫—ñ —Å–ª–æ–≤–∞-–ø–∞—Ä–∞–∑–∏—Ç–∏ —â–æ –º–æ–∂—É—Ç—å –≤—ñ–¥—Ä—ñ–∑–Ω—è—Ç–∏—Å—è
        filler_words = ['um', 'uh', 'er', 'ah', 'like', 'you know']
        words = normalized.split()
        words = [w for w in words if w not in filler_words]
        
        return ' '.join(words).strip()

    def deduplicate_with_existing_sentences(self, new_sentences: List[Dict], 
                                        video_filename: str) -> List[Dict]:
        """
        –î–µ–¥—É–ø–ª—ñ–∫—É—î –Ω–æ–≤—ñ —Ä–µ—á–µ–Ω–Ω—è –∑ –≤–∂–µ —ñ—Å–Ω—É—é—á–∏–º–∏ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö
        
        Args:
            new_sentences: –ù–æ–≤—ñ —Ä–µ—á–µ–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            video_filename: –ù–∞–∑–≤–∞ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª—É
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ—á–µ–Ω—å –∑ –ø—ñ–¥—Ç—è–≥–Ω—É—Ç–∏–º–∏ AI –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏
        """
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ —ñ—Å–Ω—É—é—á—ñ —Ä–µ—á–µ–Ω–Ω—è –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
            existing_sentences_map = self._get_existing_sentences_map()
            
            deduplicated_sentences = []
            duplicates_found = 0
            ai_reused = 0
            
            for sentence in new_sentences:
                normalized_text = self._normalize_sentence_for_comparison(sentence['text'])
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î —Ç–∞–∫–µ —Ä–µ—á–µ–Ω–Ω—è –≤ –±–∞–∑—ñ
                if normalized_text in existing_sentences_map:
                    existing_data = existing_sentences_map[normalized_text]
                    
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–µ —Ä–µ—á–µ–Ω–Ω—è –∑ —á–∞—Å–æ–≤–∏–º–∏ –º—ñ—Ç–∫–∞–º–∏ –∑ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –≤—ñ–¥–µ–æ
                    # –∞–ª–µ –∑ AI –≤—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏ –∑ —ñ—Å–Ω—É—é—á–æ–≥–æ —Ä–µ—á–µ–Ω–Ω—è
                    enhanced_sentence = sentence.copy()
                    enhanced_sentence['has_existing_ai'] = True
                    enhanced_sentence['existing_ai_source'] = existing_data['video_filename']
                    enhanced_sentence['ai_responses_count'] = existing_data['ai_count']
                    
                    duplicates_found += 1
                    if existing_data['ai_count'] > 0:
                        ai_reused += 1
                    
                    self.logger.debug(f"–ó–Ω–∞–π–¥–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç –∑ AI: {sentence['text'][:30]}... "
                                    f"(–¥–∂–µ—Ä–µ–ª–æ: {existing_data['video_filename']})")
                else:
                    enhanced_sentence = sentence.copy()
                    enhanced_sentence['has_existing_ai'] = False
                    
                deduplicated_sentences.append(enhanced_sentence)
            
            self.logger.info(f"–ì–ª–æ–±–∞–ª—å–Ω–∞ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è: –∑–Ω–∞–π–¥–µ–Ω–æ {duplicates_found} –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤, "
                            f"–ø—ñ–¥—Ç—è–≥–Ω—É—Ç–æ {ai_reused} —Ä–µ—á–µ–Ω—å –∑ AI")
            
            return deduplicated_sentences
            
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≥–ª–æ–±–∞–ª—å–Ω–æ—ó –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó: {e}")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ —Ä–µ—á–µ–Ω–Ω—è –±–µ–∑ enhancment'—É
            return new_sentences

    def _get_existing_sentences_map(self) -> Dict[str, Dict]:
        """
        –û—Ç—Ä–∏–º—É—î –º–∞–ø—É –≤—Å—ñ—Ö —ñ—Å–Ω—É—é—á–∏—Ö —Ä–µ—á–µ–Ω—å –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ {normalized_text: {video_filename, ai_count}}
        """
        try:
            sentences_map = {}
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –≤—ñ–¥–µ–æ –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
            from processing.database_manager import DatabaseManager
            db_manager = DatabaseManager()
            
            videos = db_manager.get_all_videos()
            
            for video in videos:
                try:
                    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—ñ–¥–µ–æ
                    segments = db_manager.get_video_segments(video['id'])
                    
                    # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–±—ñ—Ä –±–µ–∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó
                    video_sentences = self._split_text_simple(segments)
                    
                    for sentence in video_sentences:
                        normalized_text = self._normalize_sentence_for_comparison(sentence['text'])
                        
                        if normalized_text not in sentences_map:
                            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å–∫—ñ–ª—å–∫–∏ AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π —î –¥–ª—è —Ü—å–æ–≥–æ —Ä–µ—á–µ–Ω–Ω—è
                            ai_count = self._count_ai_responses_for_sentence(
                                sentence['text'], video['filename'], sentence['start_time']
                            )
                            
                            sentences_map[normalized_text] = {
                                'video_filename': video['filename'],
                                'ai_count': ai_count,
                                'original_text': sentence['text'],
                                'start_time': sentence['start_time']
                            }
                    
                except Exception as e:
                    self.logger.warning(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –≤—ñ–¥–µ–æ {video['filename']}: {e}")
                    continue
            
            self.logger.debug(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(sentences_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å –∑ –±–∞–∑–∏")
            return sentences_map
            
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å–Ω—É—é—á–∏—Ö —Ä–µ—á–µ–Ω—å: {e}")
            return {}

    def _split_text_simple(self, segments: List[Dict]) -> List[Dict]:
        """
        –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–±—ñ—Ä —Ç–µ–∫—Å—Ç—É –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è –ë–ï–ó –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó (–¥–ª—è –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è)
        
        Args:
            segments: –°–µ–≥–º–µ–Ω—Ç–∏ –∑ –ë–î
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ—á–µ–Ω—å –±–µ–∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó
        """
        sentences = []
        
        for segment in segments:
            text = segment['text'].strip()
            
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ñ–æ—Ä–º–∞—Ç —á–∞—Å–æ–≤–∏—Ö –º—ñ—Ç–æ–∫
            if 'start_time' in segment:
                start_time = segment['start_time']
                end_time = segment['end_time']
            elif 'start' in segment:
                start_time = segment['start']
                end_time = segment['end']
            else:
                continue
            
            # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö
            sentence_pattern = r'([.!?]+)'
            parts = re.split(sentence_pattern, text)
            
            current_sentence = ""
            sentence_start = start_time
            
            for i, part in enumerate(parts):
                if part.strip():
                    current_sentence += part
                    
                    # –Ø–∫—â–æ —Ü–µ –∫—ñ–Ω–µ—Ü—å —Ä–µ—á–µ–Ω–Ω—è
                    if re.match(sentence_pattern, part):
                        if current_sentence.strip():
                            progress = (i + 1) / len(parts)
                            sentence_end = sentence_start + (end_time - sentence_start) * progress
                            
                            sentences.append({
                                'text': current_sentence.strip(),
                                'start_time': float(sentence_start),
                                'end_time': float(min(sentence_end, end_time)),
                                'confidence': segment.get('confidence', 0.0)
                            })
                            
                            current_sentence = ""
                            sentence_start = sentence_end
            
            # –¢–µ–∫—Å—Ç –±–µ–∑ –∫—ñ–Ω—Ü–µ–≤–æ—ó –ø—É–Ω–∫—Ç—É–∞—Ü—ñ—ó
            if current_sentence.strip():
                sentences.append({
                    'text': current_sentence.strip(),
                    'start_time': float(sentence_start),
                    'end_time': float(end_time),
                    'confidence': segment.get('confidence', 0.0)
                })
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∫–æ—Ä–æ—Ç–∫—ñ —Ä–µ—á–µ–Ω–Ω—è
        return [s for s in sentences if len(s['text'].strip()) >= 10 
                and not re.match(r'^[.!?\s]+$', s['text'])]

    def _count_ai_responses_for_sentence(self, sentence_text: str, 
                                    video_filename: str, start_time: float) -> int:
        """
        –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –¥–ª—è —Ä–µ—á–µ–Ω–Ω—è
        
        Returns:
            –ö—ñ–ª—å–∫—ñ—Å—Ç—å AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π (translation + grammar + custom)
        """
        try:
            ai_count = 0
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥
            translation = self.data_manager.get_ai_response(
                sentence_text=sentence_text,
                video_filename=video_filename,
                start_time=start_time,
                response_type='translation'
            )
            if translation:
                ai_count += 1
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –≥—Ä–∞–º–∞—Ç–∏–∫—É
            grammar = self.data_manager.get_ai_response(
                sentence_text=sentence_text,
                video_filename=video_filename,
                start_time=start_time,
                response_type='grammar'
            )
            if grammar:
                ai_count += 1
            
            # –ú–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –∫–∞—Å—Ç–æ–º–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤, –∞–ª–µ —Ü–µ —Å–∫–ª–∞–¥–Ω—ñ—à–µ
            # —á–µ—Ä–µ–∑ —Ä—ñ–∑–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏
            
            return ai_count
            
        except Exception as e:
            self.logger.debug(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π: {e}")
            return 0

    def process_single_video(self, video_info: Dict) -> Dict:
        """
        –û–±—Ä–æ–±–ª—è—î –æ–¥–∏–Ω –≤—ñ–¥–µ–æ —Ñ–∞–π–ª –∑ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é —Ä–µ—á–µ–Ω—å
        
        Args:
            video_info: –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –≤—ñ–¥–µ–æ
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–æ–±–∫–∏ –∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó
        """
        start_time = time.time()
        filename = video_info["filename"]
        filepath = video_info["filepath"]
        
        result = {
            "filename": filename,
            "success": False,
            "sentences_count": 0,
            "unique_sentences": 0,
            "duplicates_found": 0,
            "ai_reused": 0,
            "processing_time": 0.0,
            "error": None
        }
        
        try:
            self.logger.info(f"–ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏: {filename}")
            
            # 1. –î–æ–¥–∞—î–º–æ –≤—ñ–¥–µ–æ –≤ –æ—Å–Ω–æ–≤–Ω—É –ë–î
            video_file_info = self.audio_extractor.get_video_info(filepath)
            duration = float(video_file_info['format']['duration']) if video_file_info else None
            file_size = int(video_file_info['format']['size']) if video_file_info else None
            
            video_id = self.db_manager.add_video(
                filename=filename,
                filepath=filepath,
                duration=duration,
                file_size=file_size
            )
            
            # 2. –í–∏—Ç—è–≥—É—î–º–æ –∞—É–¥—ñ–æ
            self.logger.info(f"–í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ: {filename}")
            audio_path = self.audio_extractor.extract_audio(filepath)
            
            if not audio_path:
                raise Exception("–ü–æ–º–∏–ª–∫–∞ –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –∞—É–¥—ñ–æ")
            
            # 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è
            self.logger.info(f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è: {filename}")
            transcription_result = self.transcriber.transcribe_audio(audio_path, language='en')
            
            if not transcription_result:
                raise Exception("–ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó")
            
            # –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥—É—î–º–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            segments = transcription_result.get("segments", [])
            if segments:
                self.logger.info(f"–ü—Ä–∏–∫–ª–∞–¥ —Å–µ–≥–º–µ–Ω—Ç—É: {segments[0]}")
                self.logger.info(f"–ö–ª—é—á—ñ —Å–µ–≥–º–µ–Ω—Ç—É: {list(segments[0].keys())}")
            
            # 4. –î–æ–¥–∞—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—é –≤ –æ—Å–Ω–æ–≤–Ω—É –ë–î
            transcription_id = self.db_manager.add_transcription(video_id, transcription_result)
            
            # 5. –†–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è –∑ –ª–æ–∫–∞–ª—å–Ω–æ—é –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—î—é
            self.logger.info(f"–†–æ–∑–±–∏–≤–∫–∞ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è: {len(segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
            sentences = self.split_text_into_sentences(segments)
            self.logger.info(f"–û—Ç—Ä–∏–º–∞–Ω–æ {len(sentences)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Ä–µ—á–µ–Ω—å –ø—ñ—Å–ª—è –ª–æ–∫–∞–ª—å–Ω–æ—ó –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—ó")
            
            # 6. –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –ì–ª–æ–±–∞–ª—å–Ω–∞ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ —Ä–µ—á–µ–Ω–Ω—è–º–∏
            self.logger.info("–ì–ª–æ–±–∞–ª—å–Ω–∞ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ —Ä–µ—á–µ–Ω–Ω—è–º–∏...")
            enhanced_sentences = self.deduplicate_with_existing_sentences(sentences, filename)
            
            # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            duplicates_found = sum(1 for s in enhanced_sentences if s.get('has_existing_ai', False))
            ai_reused = sum(1 for s in enhanced_sentences 
                        if s.get('has_existing_ai', False) and s.get('ai_responses_count', 0) > 0)
            
            result.update({
                "sentences_count": len(enhanced_sentences),
                "unique_sentences": len(enhanced_sentences) - duplicates_found,
                "duplicates_found": duplicates_found,
                "ai_reused": ai_reused
            })
            
            # –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê: –õ–æ–≥—É—î–º–æ –ø—Ä–∏–∫–ª–∞–¥ —Ä–µ—á–µ–Ω–Ω—è
            if enhanced_sentences:
                self.logger.info(f"–ü—Ä–∏–∫–ª–∞–¥ —Ä–µ—á–µ–Ω–Ω—è: {enhanced_sentences[0]}")
            
            # 7. –ù–û–í–ê –§–£–ù–ö–¶–Ü–Ø: –ö–æ–ø—ñ—é—î–º–æ AI –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
            self.copy_ai_responses_for_duplicates(enhanced_sentences, filename)
            
            # 8. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞–Ω –≤ data_manager
            state_id = self.data_manager.save_video_state(
                video_filename=filename,
                video_path=filepath,
                sentences_count=len(enhanced_sentences)
            )
            
            # 9. –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –∑–∞–≤–µ—Ä—à–µ–Ω—É –æ–±—Ä–æ–±–∫—É
            import sqlite3
            with sqlite3.connect(self.data_manager.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    UPDATE video_processing_state 
                    SET processing_completed = TRUE, sentences_extracted = ?
                    WHERE video_filename = ?
                """, (len(enhanced_sentences), filename))
                conn.commit()
            
            result.update({
                "success": True,
                "video_id": video_id,
                "transcription_id": transcription_id,
                "state_id": state_id
            })
            
            self.processing_stats["videos_processed"] += 1
            self.processing_stats["sentences_extracted"] += len(enhanced_sentences)
            
            self.logger.info(f"–û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {filename}")
            self.logger.info(f"  üìä –í—Å—å–æ–≥–æ —Ä–µ—á–µ–Ω—å: {len(enhanced_sentences)}")
            self.logger.info(f"  üÜï –ù–æ–≤–∏—Ö —Ä–µ—á–µ–Ω—å: {len(enhanced_sentences) - duplicates_found}")
            self.logger.info(f"  üîÑ –î—É–±–ª—ñ–∫–∞—Ç—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {duplicates_found}")
            self.logger.info(f"  ü§ñ AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –ø—ñ–¥—Ç—è–≥–Ω—É—Ç–æ: {ai_reused}")
            
        except Exception as e:
            error_msg = f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ {filename}: {str(e)}"
            self.logger.error(error_msg)
            
            # –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê: –ë—ñ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π –ø–æ–º–∏–ª–∫–∏
            import traceback
            self.logger.error(f"–î–µ—Ç–∞–ª—ñ –ø–æ–º–∏–ª–∫–∏:\n{traceback.format_exc()}")
            
            result["error"] = error_msg
            self.processing_stats["videos_failed"] += 1
            
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –Ω–µ–≤–¥–∞–ª—É –æ–±—Ä–æ–±–∫—É
            try:
                self.data_manager.save_video_state(
                    video_filename=filename,
                    video_path=filepath,
                    sentences_count=0
                )
            except Exception:
                pass
        
        finally:
            result["processing_time"] = time.time() - start_time
            self.processing_stats["processing_time"] += result["processing_time"]

        return result

    def copy_ai_responses_for_duplicates(self, enhanced_sentences: List[Dict], 
                                    target_video_filename: str):
        """
        –ö–æ–ø—ñ—é—î AI –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –¥–ª—è —Ä–µ—á–µ–Ω—å-–¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤ —É –Ω–æ–≤–µ –≤—ñ–¥–µ–æ
        
        Args:
            enhanced_sentences: –†–µ—á–µ–Ω–Ω—è –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
            target_video_filename: –ù–∞–∑–≤–∞ —Ü—ñ–ª—å–æ–≤–æ–≥–æ –≤—ñ–¥–µ–æ
        """
        try:
            copied_responses = 0
            
            for sentence in enhanced_sentences:
                if sentence.get('has_existing_ai', False) and sentence.get('ai_responses_count', 0) > 0:
                    source_video = sentence['existing_ai_source']
                    
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è –≤ –±–∞–∑—ñ
                    original_sentence_data = self._find_original_sentence(
                        sentence['text'], source_video
                    )
                    
                    if original_sentence_data:
                        # –ö–æ–ø—ñ—é—î–º–æ –ø–µ—Ä–µ–∫–ª–∞–¥
                        translation = self.data_manager.get_ai_response(
                            sentence_text=original_sentence_data['text'],
                            video_filename=source_video,
                            start_time=original_sentence_data['start_time'],
                            response_type='translation'
                        )
                        
                        if translation:
                            self.data_manager.save_ai_response(
                                sentence_text=sentence['text'],
                                video_filename=target_video_filename,
                                start_time=sentence['start_time'],
                                end_time=sentence['end_time'],
                                response_type='translation',
                                ai_response=translation['ai_response'],
                                ai_client=translation['ai_client']
                            )
                            copied_responses += 1
                        
                        # –ö–æ–ø—ñ—é—î–º–æ –≥—Ä–∞–º–∞—Ç–∏–∫—É
                        grammar = self.data_manager.get_ai_response(
                            sentence_text=original_sentence_data['text'],
                            video_filename=source_video,
                            start_time=original_sentence_data['start_time'],
                            response_type='grammar'
                        )
                        
                        if grammar:
                            self.data_manager.save_ai_response(
                                sentence_text=sentence['text'],
                                video_filename=target_video_filename,
                                start_time=sentence['start_time'],
                                end_time=sentence['end_time'],
                                response_type='grammar',
                                ai_response=grammar['ai_response'],
                                ai_client=grammar['ai_client']
                            )
                            copied_responses += 1
            
            if copied_responses > 0:
                self.logger.info(f"–°–∫–æ–ø—ñ–π–æ–≤–∞–Ω–æ {copied_responses} AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π –¥–ª—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤")
                
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π: {e}")

    def _find_original_sentence(self, sentence_text: str, source_video: str) -> Optional[Dict]:
        """
        –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è –≤ –±–∞–∑—ñ –¥–ª—è –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
        
        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é –ø—Ä–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–µ —Ä–µ—á–µ–Ω–Ω—è –∞–±–æ None
        """
        try:
            from processing.database_manager import DatabaseManager
            db_manager = DatabaseManager()
            
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–µ–æ
            videos = db_manager.get_all_videos()
            video = next((v for v in videos if v['filename'] == source_video), None)
            
            if not video:
                return None
            
            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–µ–≥–º–µ–Ω—Ç–∏ —Ç–∞ —Ä–æ–∑–±–∏–≤–∞—î–º–æ –Ω–∞ —Ä–µ—á–µ–Ω–Ω—è
            segments = db_manager.get_video_segments(video['id'])
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π —Ä–æ–∑–±—ñ—Ä
            sentences = self._split_text_simple(segments)
            
            # –®—É–∫–∞—î–º–æ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂–µ —Ä–µ—á–µ–Ω–Ω—è
            normalized_target = self._normalize_sentence_for_comparison(sentence_text)
            
            for sentence in sentences:
                normalized_source = self._normalize_sentence_for_comparison(sentence['text'])
                if normalized_source == normalized_target:
                    return sentence
            
            return None
            
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—á–µ–Ω–Ω—è: {e}")
            return None
    
    def process_all_videos(self, force_reprocess: bool = False) -> Dict:
        """
        –û–±—Ä–æ–±–ª—è—î –≤—Å—ñ –≤—ñ–¥–µ–æ —Ñ–∞–π–ª–∏
        
        Args:
            force_reprocess: –ß–∏ –ø–µ—Ä–µ–ø—Ä–æ—Ü–µ—Å—É–≤–∞—Ç–∏ –≤—Å—ñ —Ñ–∞–π–ª–∏
            
        Returns:
            –ó–∞–≥–∞–ª—å–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–æ–±–∫–∏
        """
        start_time = time.time()
        
        # –°–∫–∏–¥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.processing_stats = {
            "videos_found": 0,
            "videos_new": 0,
            "videos_changed": 0,
            "videos_processed": 0,
            "videos_failed": 0,
            "sentences_extracted": 0,
            "processing_time": 0.0
        }
        
        self.logger.info("–ü–æ—á–∞—Ç–æ–∫ –æ–±—Ä–æ–±–∫–∏ –≤—Å—ñ—Ö –≤—ñ–¥–µ–æ")
        
        # –°–∫–∞–Ω—É—î–º–æ –ø–∞–ø–∫—É –∑ –≤—ñ–¥–µ–æ
        video_files = self.scan_videos_directory()
        
        if not video_files:
            return {
                "success": True,
                "message": "–í—ñ–¥–µ–æ —Ñ–∞–π–ª–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω—ñ",
                "stats": self.processing_stats
            }
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–±—Ä–æ–±–∫–∏
        processing_results = []
        
        # –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω —Ñ–∞–π–ª
        for video_info in video_files:
            filename = video_info["filename"]
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–∞–Ω —Ñ–∞–π–ª—É
            if not force_reprocess:
                status = self.check_video_changes(video_info)
                
                if status == 'processed':
                    self.logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω–æ): {filename}")
                    continue
                elif status == 'new':
                    self.processing_stats["videos_new"] += 1
                elif status == 'changed':
                    self.processing_stats["videos_changed"] += 1
                    self.logger.info(f"–§–∞–π–ª –∑–º—ñ–Ω–∏–≤—Å—è: {filename}")
            
            # –û–±—Ä–æ–±–ª—è—î–º–æ –≤—ñ–¥–µ–æ
            result = self.process_single_video(video_info)
            processing_results.append(result)
        
        # –ü—ñ–¥—Å—É–º–∫–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        total_time = time.time() - start_time
        self.processing_stats["processing_time"] = total_time
        
        success_count = len([r for r in processing_results if r["success"]])
        total_sentences = sum(r["sentences_count"] for r in processing_results)
        
        summary = {
            "success": True,
            "message": f"–û–±—Ä–æ–±–ª–µ–Ω–æ {success_count}/{len(processing_results)} –≤—ñ–¥–µ–æ",
            "stats": self.processing_stats,
            "total_sentences": total_sentences,
            "total_time": total_time,
            "results": processing_results
        }
        
        self.logger.info(f"–û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count}/{len(processing_results)} —É—Å–ø—ñ—à–Ω–æ")
        self.logger.info(f"–ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å: {total_time:.1f} —Å–µ–∫—É–Ω–¥")
        self.logger.info(f"–†–µ—á–µ–Ω—å –≤–∏—Ç—è–≥–Ω—É—Ç–æ: {total_sentences}")
        
        return summary
    
    def get_processed_videos_summary(self) -> List[Dict]:
        """
        –û—Ç—Ä–∏–º—É—î –∑–≤–µ–¥–µ–Ω–Ω—è –ø–æ –≤—Å—ñ—Ö –æ–±—Ä–æ–±–ª–µ–Ω–∏—Ö –≤—ñ–¥–µ–æ
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—ñ–¥–µ–æ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
        """
        video_states = self.data_manager.get_all_video_states()
        summary = []
        
        for state in video_states:
            filename = state["video_filename"]
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å AI –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
            with self.data_manager.db_path.open() as conn:
                import sqlite3
                cursor = conn.cursor()
                
                cursor.execute("""
                    SELECT COUNT(DISTINCT sentence_hash) as sentences_with_ai
                    FROM ai_responses 
                    WHERE video_filename = ?
                """, (filename,))
                
                ai_count = cursor.fetchone()[0] if cursor.fetchone() else 0
            
            summary.append({
                "filename": filename,
                "sentences_total": state["sentences_extracted"],
                "sentences_with_ai": ai_count,
                "processing_completed": state["processing_completed"],
                "last_processed": state["last_processed"],
                "ai_coverage": round((ai_count / state["sentences_extracted"]) * 100, 1) if state["sentences_extracted"] > 0 else 0
            })
        
        return sorted(summary, key=lambda x: x["last_processed"], reverse=True)
    
    def cleanup_temp_files(self):
        """–û—á–∏—â–∞—î —Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏"""
        temp_dir = Path("temp")
        if temp_dir.exists():
            for file_path in temp_dir.iterdir():
                try:
                    if file_path.is_file():
                        file_path.unlink()
                except Exception as e:
                    self.logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥–∞–ª–∏—Ç–∏ {file_path}: {e}")

# –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
if __name__ == "__main__":
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å–æ—Ä–∞
    processor = VideoProcessor()
    
    # –¢–µ—Å—Ç —Å–∫–∞–Ω—É–≤–∞–Ω–Ω—è
    print("=== –°–∫–∞–Ω—É–≤–∞–Ω–Ω—è –≤—ñ–¥–µ–æ ===")
    videos = processor.scan_videos_directory()
    for video in videos:
        print(f"üìπ {video['filename']} ({video['size']/1024/1024:.1f} MB)")
    
    # –¢–µ—Å—Ç –æ–±—Ä–æ–±–∫–∏ –≤—Å—ñ—Ö –≤—ñ–¥–µ–æ
    print("\n=== –û–±—Ä–æ–±–∫–∞ –≤—Å—ñ—Ö –≤—ñ–¥–µ–æ ===")
    result = processor.process_all_videos()
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result['message']}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {result['stats']}")
    
    # –ó–≤–µ–¥–µ–Ω–Ω—è
    print("\n=== –ó–≤–µ–¥–µ–Ω–Ω—è –ø–æ –≤—ñ–¥–µ–æ ===")
    summary = processor.get_processed_videos_summary()
    for item in summary:
        print(f"üìπ {item['filename']}: {item['sentences_total']} —Ä–µ—á–µ–Ω—å, "
              f"{item['sentences_with_ai']} –∑ AI ({item['ai_coverage']}%)")